{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7273d179",
   "metadata": {},
   "source": [
    "## **02 - Feature Engineering (FE)**\n",
    "\n",
    "**Source**\n",
    "\n",
    "This notebook starts from the \n",
    "\n",
    "**Goals**\n",
    "- Transform the canonical EDA dataset into a model-ready feature matrix without redefining the prediction problem\n",
    "- Preserve a strict separation of concerns between:\n",
    "    - EDA (problem definition and data semantics)\n",
    "    - Feature engineering (input representation)\n",
    "    - Modeling (learning and evaluation)\n",
    "- Ensure all feature transformations are:\n",
    "    - Deterministic\n",
    "    - Reproducible\n",
    "    - Auditable\n",
    "- Minimize the risk of data leakage by:\n",
    "    - Using only information available at prediction time\n",
    "    - Applying all target-independent transformations\n",
    "- Maintain model-agnostic feature representations that can be reused across:\n",
    "    - Linear models\n",
    "    - Tree-based models\n",
    "    - Ensemble methods\n",
    "- Prioritize interpretability and transparency, especially in a credit risk context\n",
    "- Enable a clean handoff to a production-grade feature build script driven by a single feature specification\n",
    "- Support iterative experimentation through explicit versioning, rather than ad-hoc feature changes\n",
    "\n",
    "**To-Do Checklist** \n",
    "\n",
    "1. Inputs & Data Contracts\n",
    "- [] Load cleaned dataset produced by EDA\n",
    "- [] Load eda_summary.json\n",
    "- [] Verify EDA summary contains target, drop columns, numeric, categorical, and datetime columns\n",
    "- [] Validate dataset schema matches EDA expectations\n",
    "- [] Fail early if schema mismatch is detected\n",
    "\n",
    "2. Target Variable Handling\n",
    "- Confirm target column exists\n",
    "- Verify target encoding is binary\n",
    "- Validate class distribution\n",
    "- Freeze target definition\n",
    "- Exclude target from feature transforms\n",
    "\n",
    "3. Feature Exclusion Logic\n",
    "- Drop explicitly excluded columns\n",
    "- Drop constant and low-variance columns\n",
    "- Drop identifier columns\n",
    "- Verify dropped columns are not used downstream\n",
    "- Log total columns removed\n",
    "\n",
    "4. Feature Type Validation\n",
    "- Validate numeric dtypes and ranges\n",
    "- Validate categorical cardinality\n",
    "- Validate datetime parsing\n",
    "- Assert column type disjointness\n",
    "\n",
    "5. Datetime Feature Engineering\n",
    "- Convert datetime columns explicitly\n",
    "- Derive year, month, quarter features\n",
    "- Validate temporal logic\n",
    "- Drop raw datetime columns\n",
    "- Document decisions\n",
    "\n",
    "6. Categorical Feature Engineering\n",
    "- Compute cardinality\n",
    "- Assign encoding strategy by tier\n",
    "- Consolidate rare categories\n",
    "- Handle missing categories\n",
    "- Document encoding per feature\n",
    "\n",
    "7. Numerical Feature Engineering\n",
    "- Inspect distributions\n",
    "- Identify skew\n",
    "- Apply safe transforms\n",
    "- Preserve originals unless justified\n",
    "- Avoid premature normalization\n",
    "\n",
    "8. Feature Interactions (Optional)\n",
    "- Identify justified interactions\n",
    "- Avoid combinatorial explosion\n",
    "- Validate inference availability\n",
    "- Document rationale\n",
    "\n",
    "9. Dimensionality Reduction (Optional / Conditional)\n",
    "- Evaluate whether dimensionality reduction is necessary based on:\n",
    "    - Feature count after encoding\n",
    "    - Multicollinearity severity\n",
    "    - Model class requirements\n",
    "- Prefer feature selection over projection-based methods by default\n",
    "- If used:\n",
    "    - Fit dimensionality reduction on training data only\n",
    "    - Treat the reducer as part of the preprocessing pipeline\n",
    "    - Persist the fitted reducer artifact\n",
    "- Validate:\n",
    "    - Information retention\n",
    "    - Impact on downstream model performance\n",
    "    - Stability across folds\n",
    "- Document interpretability and regulatory tradeoffs\n",
    "- Explicitly justify inclusion or exclusion\n",
    "\n",
    "    Notes\n",
    "    - Projection-based methods (e.g., PCA) are generally avoided for:\n",
    "        - Linear credit models\n",
    "        - Regulated or explainability-sensitive settings\n",
    "    - Tree-based models often do not require dimensionality reduction\n",
    "\n",
    "10. Missing Value Strategy (Design Only)\n",
    "- Identify missingness\n",
    "- Define numeric and categorical strategies\n",
    "- Do not fit imputers here\n",
    "- Defer fitting to build script\n",
    "\n",
    "11. Feature Leakage Checks\n",
    "- Ensure no post-outcome features\n",
    "- Validate prediction-time availability\n",
    "- Check datetime cutoffs\n",
    "- Document assumptions\n",
    "\n",
    "12. Candidate Feature Definition & Evaluation (Design Phase)\n",
    "(Optional section â€” occurs before Feature Specification Output)\n",
    "- Enumerate candidate engineered features by type:\n",
    "    - Datetime-derived features\n",
    "    - Encoded categorical features\n",
    "    - Transformed numerical features\n",
    "    - Interaction features (if any)\n",
    "- Clearly label features as candidates, not final\n",
    "- Validate:\n",
    "    - Availability at prediction time\n",
    "    - No target leakage\n",
    "    - Interpretability\n",
    "- Evaluate candidates using:\n",
    "    - Simple univariate metrics\n",
    "    - Stability checks\n",
    "    - Sanity plots\n",
    "- Track:\n",
    "    - Included\n",
    "    - Excluded\n",
    "    - Deferred (v2)\n",
    "\n",
    "13. Feature Specification Output\n",
    "- Create feature spec\n",
    "- Include version, features, drops, transforms\n",
    "- Store under configs/\n",
    "- Treat as single source of truth\n",
    "\n",
    "14. Feature Preview Artifact (Exploratory Only)\n",
    "- Generate preview matrix\n",
    "- Validate shape, names, dtypes\n",
    "- Check NaNs/infs\n",
    "- Save with version tag\n",
    "- Mark as non-authoritative\n",
    "\n",
    "15. Validation & Sanity Checks\n",
    "- No duplicates\n",
    "- No unexpected NaNs or infs\n",
    "- Reasonable feature count\n",
    "- Target distribution unchanged\n",
    "\n",
    "16. Reproducibility & Versioning\n",
    "- Record version\n",
    "- Freeze seeds if applicable\n",
    "- Log feature counts\n",
    "- Store metadata\n",
    "\n",
    "17. Handoff to Feature Build Script\n",
    "- Identify logic to move to src/features\n",
    "- Confirm script inputs and outputs\n",
    "- Ensure notebook not required for rebuild\n",
    "\n",
    "18. Notebook Completion Criteria\n",
    "- Decisions documented\n",
    "- Spec finalized\n",
    "- No hidden logic\n",
    "- Rebuild reproducible via script\n",
    "\n",
    "19. Reviewer-Facing Summary\n",
    "- Why these features\n",
    "- Tradeoffs\n",
    "- Limitations\n",
    "- Planned v2 changes\n",
    "\n",
    "---\n",
    "\n",
    "Definition of Done  \n",
    "A model can be trained without this notebook using only the feature spec and the feature build script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e6f5da",
   "metadata": {},
   "source": [
    "# 1. Inputs & Data Contracts\n",
    "\n",
    "We will:\n",
    "\n",
    "- Load cleaned dataset produced by EDA\n",
    "- Load eda_summary.json\n",
    "- Verify EDA summary contains target, drop columns, numeric, categorical, and datetime columns\n",
    "- Validate dataset schema matches EDA expectations\n",
    "- Fail early if schema mismatch is detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707e2fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected feature columns: 77\n",
      "Expected feature columns sample: ['acc_open_past_24mths', 'addr_state', 'all_util', 'annual_inc', 'annual_inc_joint', 'application_type', 'avg_cur_bal', 'bc_open_to_buy', 'bc_util', 'delinq_2yrs', 'dti', 'dti_joint', 'emp_length', 'fico_range_high', 'fico_range_low', 'funded_amnt', 'grade', 'home_ownership', 'il_util', 'inq_fi', 'installment', 'int_rate', 'issue_d', 'loan_amnt', 'max_bal_bc', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mo_sin_rcnt_tl', 'mort_acc', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl', 'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_sats', 'num_tl_120dpd_2m', 'num_tl_90g_dpd_24m', 'num_tl_op_past_12m', 'open_acc', 'open_acc_6m', 'open_act_il', 'open_il_12m', 'open_il_24m', 'open_rv_12m', 'open_rv_24m', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'pub_rec', 'pub_rec_bankruptcies', 'purpose', 'revol_bal', 'revol_bal_joint', 'sec_app_chargeoff_within_12_mths', 'sec_app_fico_range_high', 'sec_app_fico_range_low', 'sec_app_mort_acc', 'sec_app_num_rev_accts', 'sec_app_open_acc', 'sec_app_open_act_il', 'sub_grade', 'tax_liens', 'term', 'tot_coll_amt', 'tot_cur_bal', 'tot_hi_cred_lim', 'total_acc', 'total_bal_ex_mort', 'total_bal_il', 'total_bc_limit', 'total_cu_tl', 'total_il_high_credit_limit', 'total_rev_hi_lim', 'zip_code']\n",
      "Actual feature columns: 77\n",
      "Column not in actual dataset: set()\n",
      "Feature Engineering input loaded successfully\n",
      "X shape: (2252232, 77)\n",
      "y shape: (2252232,)\n",
      "Target distribution:\n",
      "default\n",
      "0    0.868941\n",
      "1    0.131059\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Paths (single source of truth)\n",
    "# -----------------------------\n",
    "DATA_DIR = Path(\"../data/_artifacts_preview\")\n",
    "\n",
    "EDA_DATA_PATH = DATA_DIR / \"eda_cleaned.parquet\"\n",
    "EDA_SCHEMA_PATH = DATA_DIR / \"eda_cleaned_schema.json\"\n",
    "EDA_SUMMARY_PATH = DATA_DIR / \"eda_summary.json\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load artifacts\n",
    "# -----------------------------\n",
    "if not EDA_DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing EDA dataset: {EDA_DATA_PATH}\")\n",
    "\n",
    "if not EDA_SCHEMA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing EDA schema: {EDA_SCHEMA_PATH}\")\n",
    "\n",
    "if not EDA_SUMMARY_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing EDA summary: {EDA_SUMMARY_PATH}\")\n",
    "\n",
    "df = pd.read_parquet(EDA_DATA_PATH)\n",
    "\n",
    "with open(EDA_SCHEMA_PATH, \"r\") as f:\n",
    "    eda_schema = json.load(f)\n",
    "\n",
    "with open(EDA_SUMMARY_PATH, \"r\") as f:\n",
    "    eda_summary = json.load(f)\n",
    "\n",
    "# -----------------------------\n",
    "# Contract validation (lightweight)\n",
    "# -----------------------------\n",
    "TARGET = eda_summary[\"target_variable\"]\n",
    "\n",
    "if TARGET not in df.columns:\n",
    "    raise KeyError(f\"Target column '{TARGET}' not found in EDA dataset\")\n",
    "\n",
    "num_minus_default_cols = [col for col in eda_summary[\"numerical_cols\"] if col != TARGET]\n",
    "\n",
    "expected_feature_cols = (\n",
    "    set(eda_summary[\"numerical_cols\"])\n",
    "    | set(eda_summary[\"categorical_cols\"])\n",
    "    | set(eda_summary[\"datetime_cols\"])\n",
    ")\n",
    "\n",
    "expected_feature_cols -= {TARGET}  # ensure target is not in features\n",
    "\n",
    "columns_to_drop = (\n",
    "    set(eda_summary[\"cols_to_drop\"])\n",
    "    | set(eda_summary[\"low_variance_cols\"])\n",
    "    | set(eda_summary[\"constant_cols\"])\n",
    ")\n",
    "\n",
    "expected_feature_cols -= set(columns_to_drop)\n",
    "\n",
    "actual_feature_cols = set(df.columns) - {TARGET}\n",
    "\n",
    "missing_cols = expected_feature_cols - actual_feature_cols\n",
    "extra_cols = actual_feature_cols - expected_feature_cols\n",
    "\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing expected feature columns: {sorted(missing_cols)}\")\n",
    "\n",
    "if extra_cols:\n",
    "    raise ValueError(f\"Unexpected extra columns in dataset: {sorted(extra_cols)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Separate features and target\n",
    "# -----------------------------\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET]\n",
    "\n",
    "print(\"Feature Engineering input loaded successfully\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(\"Target distribution:\")\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676cc57",
   "metadata": {},
   "source": [
    "# 12. Candidate Feature Definition & Evaluation (Design Stage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = df.copy()\n",
    "\n",
    "# Loan-to-income ratio\n",
    "if {\"loan_amnt\", \"annual_inc\"}.issubset(df_fe.columns):\n",
    "    df_fe[\"loan_to_income\"] = df_fe[\"loan_amnt\"] / df_fe[\"annual_inc\"]\n",
    "\n",
    "# Installment-to-income ratio\n",
    "if {\"installment\", \"annual_inc\"}.issubset(df_fe.columns):\n",
    "    df_fe[\"installment_to_income\"] = df_fe[\"installment\"] / df_fe[\"annual_inc\"]\n",
    "\n",
    "# Term in months\n",
    "if \"term\" in df_fe.columns:\n",
    "    df_fe[\"term_months\"] = df_fe[\"term\"].astype(str).str.extract(r\"(\\d+)\").astype(float)\n",
    "\n",
    "# Grade numeric\n",
    "if \"grade\" in df_fe.columns:\n",
    "    grade_map = {g: i for i, g in enumerate(sorted(df_fe[\"grade\"].dropna().unique()))}\n",
    "    df_fe[\"grade_numeric\"] = df_fe[\"grade\"].map(grade_map)\n",
    "\n",
    "# Subgrade numeric (e.g., A1, B3)\n",
    "if \"sub_grade\" in df_fe.columns:\n",
    "    sub_sorted = sorted(df_fe[\"sub_grade\"].dropna().unique())\n",
    "    sub_map = {sg: i for i, sg in enumerate(sub_sorted)}\n",
    "    df_fe[\"sub_grade_numeric\"] = df_fe[\"sub_grade\"].map(sub_map)\n",
    "\n",
    "# Log transforms for skewed variables\n",
    "for col in [\"annual_inc\", \"loan_amnt\"]:\n",
    "    if col in df_fe.columns:\n",
    "        df_fe[f\"log_{col}\"] = np.log1p(df_fe[col])\n",
    "\n",
    "df_fe.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-risk-pd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
