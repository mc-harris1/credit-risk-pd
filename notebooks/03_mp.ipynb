{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0fdd07a",
   "metadata": {},
   "source": [
    "# 03 — Modeling & Performance (MP)\n",
    "\n",
    "This notebook is **contract-driven** and assumes upstream steps have been refactored:\n",
    "\n",
    "- `src/data/preprocess.py` → produces `data/interim/loans_cleaned.parquet`\n",
    "- `src/features/build_features.py` → produces `data/processed/engineered_features_v1.parquet`\n",
    "- `notebooks/02_fe.ipynb` → produces `data/artifacts/feature_spec_v1.json`\n",
    "\n",
    "**Inputs**\n",
    "- `data/processed/engineered_features_v1.parquet`\n",
    "- `data/artifacts/feature_spec_v1.json`\n",
    "\n",
    "**Outputs**\n",
    "- `models/bundles/<bundle_id>/model.joblib`\n",
    "- `models/bundles/<bundle_id>/metadata.json`\n",
    "- `models/bundles/<bundle_id>/feature_spec_v1.json` (copied for provenance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64292f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "from typing import Any, List\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, brier_score_loss, log_loss, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Calibration: sklearn >=1.6 uses FrozenEstimator for prefit; fall back gracefully if unavailable.\n",
    "try:\n",
    "    from sklearn.frozen import FrozenEstimator\n",
    "except Exception:  # pragma: no cover\n",
    "    FrozenEstimator = None  # type: ignore\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent  # notebooks/ -> repo root\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "ARTIFACTS_DIR = DATA_DIR / \"artifacts\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "BUNDLES_DIR = PROJECT_ROOT / \"models\" / \"bundles\"\n",
    "\n",
    "FEATURE_MATRIX_PATH = PROCESSED_DIR / \"engineered_features_v1.parquet\"\n",
    "FEATURE_SPEC_PATH = ARTIFACTS_DIR / \"feature_spec_v1.json\"\n",
    "\n",
    "assert FEATURE_MATRIX_PATH.exists(), f\"Missing: {FEATURE_MATRIX_PATH}\"\n",
    "assert FEATURE_SPEC_PATH.exists(), f\"Missing: {FEATURE_SPEC_PATH}\"\n",
    "\n",
    "\n",
    "def read_json(path: Path) -> Any:\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "\n",
    "def sha256_bytes(b: bytes) -> str:\n",
    "    return hashlib.sha256(b).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b28b2d2",
   "metadata": {},
   "source": [
    "## 1) Load feature matrix + enforce the feature contract\n",
    "\n",
    "We use `feature_spec_v1.json` to determine:\n",
    "- `TARGET` (`spec.target.name`)\n",
    "- the contracted feature columns, inferred from the spec + observed engineered matrix columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e6ead00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_matrix shape: (1370163, 36)\n",
      "target: default\n",
      "X shape: (1370163, 31)\n",
      "Positive rate: 0.21465110355483252\n"
     ]
    }
   ],
   "source": [
    "spec = read_json(FEATURE_SPEC_PATH)\n",
    "TARGET = spec[\"target\"][\"name\"]\n",
    "ALLOWED_TARGET_VALUES = set(spec[\"target\"].get(\"allowed_values\", [0, 1]))\n",
    "\n",
    "df = pd.read_parquet(FEATURE_MATRIX_PATH)\n",
    "print(\"feature_matrix shape:\", df.shape)\n",
    "print(\"target:\", TARGET)\n",
    "\n",
    "if TARGET not in df.columns:\n",
    "    raise KeyError(f\"Target '{TARGET}' not present in feature matrix.\")\n",
    "\n",
    "y = df[TARGET]\n",
    "if y.isna().any():\n",
    "    raise ValueError(\n",
    "        \"Target contains missing values; upstream preprocessing should have excluded unmapped rows.\"\n",
    "    )\n",
    "\n",
    "unique_vals = set(int(v) for v in y.unique())\n",
    "if not unique_vals.issubset(ALLOWED_TARGET_VALUES):\n",
    "    raise ValueError(\n",
    "        f\"Target values {sorted(unique_vals)} not subset of allowed {sorted(ALLOWED_TARGET_VALUES)}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Build contracted feature list from the spec + observed engineered matrix\n",
    "# ----------------------------\n",
    "\n",
    "# 1) datetime derived features\n",
    "datetime_feats: List[str] = []\n",
    "for _col, cfg in spec.get(\"features\", {}).get(\"datetime\", {}).items():\n",
    "    datetime_feats.extend(cfg.get(\"derived_features\", []))\n",
    "\n",
    "# 2) engineered features are named keys in spec.features.engineered\n",
    "engineered_feats = list(spec.get(\"features\", {}).get(\"engineered\", {}).keys())\n",
    "\n",
    "# 3) categorical outputs:\n",
    "categorical_cfg = spec.get(\"features\", {}).get(\"categorical\", {})\n",
    "\n",
    "cat_one_hot_cols: List[str] = []\n",
    "cat_scalar_cols: List[str] = []\n",
    "for col, cfg in categorical_cfg.items():\n",
    "    strat = cfg.get(\"encoding_strategy\")\n",
    "    if strat == \"one_hot\":\n",
    "        cat_one_hot_cols.extend([c for c in df.columns if c.startswith(f\"{col}_\")])\n",
    "    elif strat == \"target_mean\":\n",
    "        cat_scalar_cols.append(f\"{col}__target_mean\")\n",
    "    else:\n",
    "        cat_scalar_cols.append(f\"{col}__freq\")\n",
    "\n",
    "# 4) numerical outputs: original numeric + optional transformed columns (log1p)\n",
    "numerical_cfg = spec.get(\"features\", {}).get(\"numerical\", {})\n",
    "num_cols = list(numerical_cfg.keys())\n",
    "num_transformed = []\n",
    "for col, cfg in numerical_cfg.items():\n",
    "    if cfg.get(\"planned_transformation\") == \"log\":\n",
    "        num_transformed.append(f\"{col}__log1p\")\n",
    "\n",
    "FEATURE_COLS = sorted(\n",
    "    set(\n",
    "        datetime_feats\n",
    "        + engineered_feats\n",
    "        + cat_one_hot_cols\n",
    "        + cat_scalar_cols\n",
    "        + num_cols\n",
    "        + num_transformed\n",
    "    )\n",
    ")\n",
    "\n",
    "missing_features = [c for c in FEATURE_COLS if c not in df.columns]\n",
    "if missing_features:\n",
    "    raise ValueError(\n",
    "        f\"Feature matrix missing {len(missing_features)} contracted features. Example: {missing_features[:25]}\"\n",
    "    )\n",
    "\n",
    "X = df[FEATURE_COLS].copy()\n",
    "y = df[TARGET].astype(int).copy()\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Positive rate:\", float(y.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520c85d9",
   "metadata": {},
   "source": [
    "## 2) Construct an anchor timestamp for temporal splitting\n",
    "\n",
    "Upstream spec drops raw `issue_d`. We reconstruct an anchor using the derived datetime features:\n",
    "- `issue_d_year` + `issue_d_month` (preferred)\n",
    "- else `issue_d_year` + `issue_d_quarter`\n",
    "\n",
    "If neither exists, we fall back to row order (not ideal for time-series evaluation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e0b9607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-06-01 00:00:00 2018-12-01 00:00:00\n"
     ]
    }
   ],
   "source": [
    "def make_anchor_timestamp(df_all: pd.DataFrame) -> pd.Series:\n",
    "    if \"issue_d_year\" in df_all.columns and \"issue_d_month\" in df_all.columns:\n",
    "        y = df_all[\"issue_d_year\"].astype(\"Int64\")\n",
    "        m = df_all[\"issue_d_month\"].astype(\"Int64\")\n",
    "        return pd.to_datetime(pd.DataFrame({\"year\": y, \"month\": m, \"day\": 1}), errors=\"raise\")\n",
    "    if \"issue_d_year\" in df_all.columns and \"issue_d_quarter\" in df_all.columns:\n",
    "        y = df_all[\"issue_d_year\"].astype(\"Int64\")\n",
    "        q = df_all[\"issue_d_quarter\"].astype(\"Int64\")\n",
    "        m = (q - 1) * 3 + 1\n",
    "        return pd.to_datetime(pd.DataFrame({\"year\": y, \"month\": m, \"day\": 1}), errors=\"raise\")\n",
    "    return pd.Series(pd.RangeIndex(len(df_all)), index=df_all.index)\n",
    "\n",
    "\n",
    "df = df.copy()\n",
    "df[\"_anchor\"] = make_anchor_timestamp(df)\n",
    "print(df[\"_anchor\"].min(), df[\"_anchor\"].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d897a",
   "metadata": {},
   "source": [
    "## 3) Time-based split (train/val/test)\n",
    "\n",
    "We sort by `_anchor` to avoid temporal leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1987f112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_rows': 959114,\n",
       " 'val_rows': 205524,\n",
       " 'test_rows': 205525,\n",
       " 'train_anchor_min': '2007-06-01 00:00:00',\n",
       " 'train_anchor_max': '2016-04-01 00:00:00',\n",
       " 'val_anchor_min': '2016-04-01 00:00:00',\n",
       " 'val_anchor_max': '2017-03-01 00:00:00',\n",
       " 'test_anchor_min': '2017-03-01 00:00:00',\n",
       " 'test_anchor_max': '2018-12-01 00:00:00',\n",
       " 'train_default_rate': 0.19081881820096463,\n",
       " 'val_default_rate': 0.26563807633171793,\n",
       " 'test_default_rate': 0.2748814012893809}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def time_based_split(\n",
    "    df_all: pd.DataFrame,\n",
    "    X_all: pd.DataFrame,\n",
    "    y_all: pd.Series,\n",
    "    anchor_col: str,\n",
    "    train_frac: float = 0.70,\n",
    "    val_frac: float = 0.15,\n",
    "    test_frac: float = 0.15,\n",
    "):\n",
    "    if abs(train_frac + val_frac + test_frac - 1.0) > 1e-9:\n",
    "        raise ValueError(\"train/val/test fractions must sum to 1.0\")\n",
    "\n",
    "    order = np.argsort(df_all[anchor_col].to_numpy())\n",
    "    df_s = df_all.iloc[order].reset_index(drop=True)\n",
    "    X_s = X_all.iloc[order].reset_index(drop=True)\n",
    "    y_s = y_all.iloc[order].reset_index(drop=True)\n",
    "\n",
    "    n = len(df_s)\n",
    "    n_train = int(n * train_frac)\n",
    "    n_val = int(n * val_frac)\n",
    "\n",
    "    idx_train = slice(0, n_train)\n",
    "    idx_val = slice(n_train, n_train + n_val)\n",
    "    idx_test = slice(n_train + n_val, n)\n",
    "\n",
    "    def pack(slc):\n",
    "        return df_s.iloc[slc], X_s.iloc[slc], y_s.iloc[slc]\n",
    "\n",
    "    df_train, X_train, y_train = pack(idx_train)\n",
    "    df_val, X_val, y_val = pack(idx_val)\n",
    "    df_test, X_test, y_test = pack(idx_test)\n",
    "\n",
    "    def rate(s: pd.Series) -> float:\n",
    "        return float(pd.Series(s).astype(int).mean()) if len(s) else 0.0\n",
    "\n",
    "    meta = {\n",
    "        \"train_rows\": len(df_train),\n",
    "        \"val_rows\": len(df_val),\n",
    "        \"test_rows\": len(df_test),\n",
    "        \"train_anchor_min\": str(df_train[anchor_col].min()),\n",
    "        \"train_anchor_max\": str(df_train[anchor_col].max()),\n",
    "        \"val_anchor_min\": str(df_val[anchor_col].min()),\n",
    "        \"val_anchor_max\": str(df_val[anchor_col].max()),\n",
    "        \"test_anchor_min\": str(df_test[anchor_col].min()),\n",
    "        \"test_anchor_max\": str(df_test[anchor_col].max()),\n",
    "        \"train_default_rate\": rate(y_train),\n",
    "        \"val_default_rate\": rate(y_val),\n",
    "        \"test_default_rate\": rate(y_test),\n",
    "    }\n",
    "    return df_train, X_train, y_train, df_val, X_val, y_val, df_test, X_test, y_test, meta\n",
    "\n",
    "\n",
    "df_train, X_train, y_train, df_val, X_val, y_val, df_test, X_test, y_test, split_meta = (\n",
    "    time_based_split(df, X, y, anchor_col=\"_anchor\")\n",
    ")\n",
    "split_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90dc5a7",
   "metadata": {},
   "source": [
    "## 4) Guardrail: drop all-null columns (train-derived)\n",
    "\n",
    "Some columns can be all-null in the training window. We derive the drop list from train only and apply to val/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84184507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped all-null cols: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def drop_all_null_cols_train_only(Xtr: pd.DataFrame, *others: pd.DataFrame):\n",
    "    all_null = [c for c in Xtr.columns if Xtr[c].isna().all()]\n",
    "    Xtr2 = Xtr.drop(columns=all_null)\n",
    "    outs = [Xtr2]\n",
    "    for Xo in others:\n",
    "        outs.append(Xo.drop(columns=[c for c in all_null if c in Xo.columns]))\n",
    "    return all_null, outs\n",
    "\n",
    "\n",
    "all_null_cols, (Xtr, Xva, Xte) = drop_all_null_cols_train_only(X_train, X_val, X_test)\n",
    "print(\"Dropped all-null cols:\", len(all_null_cols))\n",
    "all_null_cols[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf893b",
   "metadata": {},
   "source": [
    "## 5) Baseline model: Logistic Regression\n",
    "\n",
    "The engineered feature matrix is numeric already, so the pipeline is:\n",
    "- median imputation\n",
    "- scaling\n",
    "- logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3d2bca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mcharris/Developer/mc-harris1/credit-risk-pd/.venv/lib/python3.13/site-packages/sklearn/linear_model/_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.6781507232790471,\n",
       " 'pr_auc': 0.4181782189708343,\n",
       " 'brier': 0.24559626173866972,\n",
       " 'log_loss': 0.6888657027258445}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scaler\", StandardScaler(with_mean=False)),\n",
    "        (\"clf\", LogisticRegression(max_iter=400, class_weight=\"balanced\", random_state=42)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "baseline.fit(Xtr, y_train.to_numpy())\n",
    "pva_lr = baseline.predict_proba(Xva)[:, 1]\n",
    "\n",
    "baseline_val_metrics = {\n",
    "    \"roc_auc\": float(roc_auc_score(y_val, pva_lr)),\n",
    "    \"pr_auc\": float(average_precision_score(y_val, pva_lr)),\n",
    "    \"brier\": float(brier_score_loss(y_val, pva_lr)),\n",
    "    \"log_loss\": float(log_loss(y_val, pva_lr)),\n",
    "}\n",
    "baseline_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d33920f",
   "metadata": {},
   "source": [
    "## 6) Primary model: HistGradientBoostingClassifier\n",
    "\n",
    "We still impute missing values. HGB is often a strong baseline for tabular risk modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17f94ae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.6870778716927307,\n",
       " 'pr_auc': 0.43110134135465245,\n",
       " 'brier': 0.1786850873834222,\n",
       " 'log_loss': 0.535503157477358}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgb = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\n",
    "            \"clf\",\n",
    "            HistGradientBoostingClassifier(\n",
    "                random_state=42,\n",
    "                max_depth=6,\n",
    "                learning_rate=0.05,\n",
    "                max_iter=300,\n",
    "                min_samples_leaf=30,\n",
    "                early_stopping=True,\n",
    "                scoring=\"loss\",\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hgb.fit(Xtr, y_train.to_numpy())\n",
    "pva_hgb = hgb.predict_proba(Xva)[:, 1]\n",
    "\n",
    "primary_val_metrics = {\n",
    "    \"roc_auc\": float(roc_auc_score(y_val, pva_hgb)),\n",
    "    \"pr_auc\": float(average_precision_score(y_val, pva_hgb)),\n",
    "    \"brier\": float(brier_score_loss(y_val, pva_hgb)),\n",
    "    \"log_loss\": float(log_loss(y_val, pva_hgb)),\n",
    "}\n",
    "primary_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f75c5",
   "metadata": {},
   "source": [
    "## 7) Test evaluation (first touch)\n",
    "\n",
    "We evaluate once on test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d353825c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'roc_auc': 0.6957195818441747,\n",
       " 'pr_auc': 0.44281279473944746,\n",
       " 'brier': 0.1809689975422927,\n",
       " 'log_loss': 0.5395366615561296}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pte_hgb = hgb.predict_proba(Xte)[:, 1]\n",
    "\n",
    "test_metrics_raw = {\n",
    "    \"roc_auc\": float(roc_auc_score(y_test, pte_hgb)),\n",
    "    \"pr_auc\": float(average_precision_score(y_test, pte_hgb)),\n",
    "    \"brier\": float(brier_score_loss(y_test, pte_hgb)),\n",
    "    \"log_loss\": float(log_loss(y_test, pte_hgb)),\n",
    "}\n",
    "test_metrics_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe1ae29",
   "metadata": {},
   "source": [
    "## 8) Optional calibration (sigmoid)\n",
    "\n",
    "We fit a calibrator on the **validation** set only.\n",
    "\n",
    "If `FrozenEstimator` is unavailable, we skip calibration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bef22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated = None\n",
    "calibration_metrics = None\n",
    "\n",
    "if FrozenEstimator is None:\n",
    "    print(\"FrozenEstimator not available in this sklearn version; skipping calibration.\")\n",
    "else:\n",
    "    cal = CalibratedClassifierCV(FrozenEstimator(hgb), method=\"sigmoid\", cv=\"prefit\")\n",
    "    cal.fit(Xva, y_val.to_numpy())\n",
    "    pte_cal = cal.predict_proba(Xte)[:, 1]\n",
    "\n",
    "    calibration_metrics = {\n",
    "        \"raw_test_brier\": float(brier_score_loss(y_test, pte_hgb)),\n",
    "        \"cal_test_brier\": float(brier_score_loss(y_test, pte_cal)),\n",
    "    }\n",
    "    calibrated = cal\n",
    "    calibration_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf06db06",
   "metadata": {},
   "source": [
    "## 9) Freeze + save model bundle\n",
    "\n",
    "We save:\n",
    "- `model.joblib`\n",
    "- `metadata.json`\n",
    "- `feature_spec_v1.json` (copy)\n",
    "\n",
    "Bundle ID is time-stamped for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ed38d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bundle: /Users/mcharris/Developer/mc-harris1/credit-risk-pd/models/bundles/pd_hgb_mp__20251218T203133Z\n"
     ]
    }
   ],
   "source": [
    "BUNDLES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "spec_hash = sha256_bytes(FEATURE_SPEC_PATH.read_bytes())\n",
    "bundle_id = f\"pd_hgb_mp__{datetime.now(timezone.utc).strftime('%Y%m%dT%H%M%SZ')}\"\n",
    "bundle_dir = BUNDLES_DIR / bundle_id\n",
    "bundle_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# select final model\n",
    "final_model = hgb\n",
    "final_name = \"hgb_uncalibrated\"\n",
    "if calibration_metrics is not None and calibrated is not None:\n",
    "    if calibration_metrics[\"cal_test_brier\"] < calibration_metrics[\"raw_test_brier\"]:\n",
    "        final_model = calibrated\n",
    "        final_name = \"hgb_sigmoid_calibrated\"\n",
    "\n",
    "joblib.dump(final_model, bundle_dir / \"model.joblib\")\n",
    "\n",
    "metadata = {\n",
    "    \"bundle_id\": bundle_id,\n",
    "    \"model_name\": final_name,\n",
    "    \"created_at_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"inputs\": {\n",
    "        \"feature_matrix\": str(FEATURE_MATRIX_PATH),\n",
    "        \"feature_spec\": str(FEATURE_SPEC_PATH),\n",
    "    },\n",
    "    \"feature_spec_hash_sha256\": spec_hash,\n",
    "    \"split_meta\": split_meta,\n",
    "    \"dropped_all_null_cols\": all_null_cols,\n",
    "    \"feature_cols\": list(Xtr.columns),\n",
    "    \"metrics\": {\n",
    "        \"baseline_val\": baseline_val_metrics,\n",
    "        \"primary_val\": primary_val_metrics,\n",
    "        \"test_raw\": test_metrics_raw,\n",
    "        \"calibration\": calibration_metrics,\n",
    "    },\n",
    "}\n",
    "\n",
    "(bundle_dir / \"metadata.json\").write_text(json.dumps(metadata, indent=2), encoding=\"utf-8\")\n",
    "(bundle_dir / \"feature_spec_v1.json\").write_text(json.dumps(spec, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved bundle:\", bundle_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a682cef",
   "metadata": {},
   "source": [
    "Permutation Importance Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ebe2fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "feature",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "importance_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "importance_std",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "d1b88b6d-cc97-4f0d-b259-b697992ab903",
       "rows": [
        [
         "26",
         "sub_grade__target_mean",
         "0.057830749517204946",
         "0.00015457533816622856"
        ],
        [
         "25",
         "loan_amnt",
         "0.00811851046879366",
         "0.00019260054832580592"
        ],
        [
         "2",
         "dti",
         "0.006697387252410802",
         "0.00018406744220382794"
        ],
        [
         "3",
         "emp_length__target_mean",
         "0.0060191265858750235",
         "0.0002610361728155619"
        ],
        [
         "19",
         "home_ownership_RENT",
         "0.004563915169845801",
         "0.00018865323001093838"
        ],
        [
         "15",
         "home_ownership_MORTGAGE",
         "0.004260504322749692",
         "8.490067470180554e-05"
        ],
        [
         "27",
         "term_ 36 months",
         "0.003790454785529085",
         "0.00020767316338557066"
        ],
        [
         "0",
         "annual_inc",
         "0.003278026276933366",
         "0.00022198853961947424"
        ],
        [
         "12",
         "grade_numeric",
         "0.003140798523574029",
         "9.248881743248787e-05"
        ],
        [
         "20",
         "int_rate",
         "0.0028294251948289386",
         "0.00014724063757033"
        ],
        [
         "5",
         "grade_A",
         "0.0004942605634916664",
         "6.03108671159331e-05"
        ],
        [
         "7",
         "grade_C",
         "0.00026283332672658586",
         "3.0399698689772594e-05"
        ],
        [
         "21",
         "issue_d_month",
         "0.00010111165767401342",
         "5.448748880471986e-06"
        ],
        [
         "6",
         "grade_B",
         "2.6927723465641763e-05",
         "1.4879583871573463e-05"
        ],
        [
         "10",
         "grade_F",
         "2.6278740421936677e-06",
         "2.763370130428353e-07"
        ],
        [
         "8",
         "grade_D",
         "2.426635415053724e-06",
         "1.4203118336603967e-06"
        ],
        [
         "11",
         "grade_G",
         "5.1505049669664515e-08",
         "4.1012972819140006e-07"
        ],
        [
         "16",
         "home_ownership_NONE",
         "0.0",
         "0.0"
        ],
        [
         "17",
         "home_ownership_OTHER",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "annual_inc__log1p",
         "0.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance_mean</th>\n",
       "      <th>importance_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sub_grade__target_mean</td>\n",
       "      <td>5.783075e-02</td>\n",
       "      <td>1.545753e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>loan_amnt</td>\n",
       "      <td>8.118510e-03</td>\n",
       "      <td>1.926005e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dti</td>\n",
       "      <td>6.697387e-03</td>\n",
       "      <td>1.840674e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>emp_length__target_mean</td>\n",
       "      <td>6.019127e-03</td>\n",
       "      <td>2.610362e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>home_ownership_RENT</td>\n",
       "      <td>4.563915e-03</td>\n",
       "      <td>1.886532e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>home_ownership_MORTGAGE</td>\n",
       "      <td>4.260504e-03</td>\n",
       "      <td>8.490067e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>term_ 36 months</td>\n",
       "      <td>3.790455e-03</td>\n",
       "      <td>2.076732e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>annual_inc</td>\n",
       "      <td>3.278026e-03</td>\n",
       "      <td>2.219885e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>grade_numeric</td>\n",
       "      <td>3.140799e-03</td>\n",
       "      <td>9.248882e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>int_rate</td>\n",
       "      <td>2.829425e-03</td>\n",
       "      <td>1.472406e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>grade_A</td>\n",
       "      <td>4.942606e-04</td>\n",
       "      <td>6.031087e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>grade_C</td>\n",
       "      <td>2.628333e-04</td>\n",
       "      <td>3.039970e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>issue_d_month</td>\n",
       "      <td>1.011117e-04</td>\n",
       "      <td>5.448749e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>grade_B</td>\n",
       "      <td>2.692772e-05</td>\n",
       "      <td>1.487958e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>grade_F</td>\n",
       "      <td>2.627874e-06</td>\n",
       "      <td>2.763370e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>grade_D</td>\n",
       "      <td>2.426635e-06</td>\n",
       "      <td>1.420312e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>grade_G</td>\n",
       "      <td>5.150505e-08</td>\n",
       "      <td>4.101297e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>home_ownership_NONE</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>home_ownership_OTHER</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>annual_inc__log1p</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    feature  importance_mean  importance_std\n",
       "26   sub_grade__target_mean     5.783075e-02    1.545753e-04\n",
       "25                loan_amnt     8.118510e-03    1.926005e-04\n",
       "2                       dti     6.697387e-03    1.840674e-04\n",
       "3   emp_length__target_mean     6.019127e-03    2.610362e-04\n",
       "19      home_ownership_RENT     4.563915e-03    1.886532e-04\n",
       "15  home_ownership_MORTGAGE     4.260504e-03    8.490067e-05\n",
       "27          term_ 36 months     3.790455e-03    2.076732e-04\n",
       "0                annual_inc     3.278026e-03    2.219885e-04\n",
       "12            grade_numeric     3.140799e-03    9.248882e-05\n",
       "20                 int_rate     2.829425e-03    1.472406e-04\n",
       "5                   grade_A     4.942606e-04    6.031087e-05\n",
       "7                   grade_C     2.628333e-04    3.039970e-05\n",
       "21            issue_d_month     1.011117e-04    5.448749e-06\n",
       "6                   grade_B     2.692772e-05    1.487958e-05\n",
       "10                  grade_F     2.627874e-06    2.763370e-07\n",
       "8                   grade_D     2.426635e-06    1.420312e-06\n",
       "11                  grade_G     5.150505e-08    4.101297e-07\n",
       "16      home_ownership_NONE     0.000000e+00    0.000000e+00\n",
       "17     home_ownership_OTHER     0.000000e+00    0.000000e+00\n",
       "1         annual_inc__log1p     0.000000e+00    0.000000e+00"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "perm = permutation_importance(\n",
    "    hgb,\n",
    "    Xva,\n",
    "    y_val,\n",
    "    n_repeats=5,\n",
    "    random_state=42,\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "importances = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": Xva.columns,\n",
    "        \"importance_mean\": perm.importances_mean,\n",
    "        \"importance_std\": perm.importances_std,\n",
    "    }\n",
    ").sort_values(\"importance_mean\", ascending=False)\n",
    "importances.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba587f",
   "metadata": {},
   "source": [
    "Permutation importance inspection revealed no evidence of target leakage or post-outcome signal. Top features align with standard credit risk drivers (pricing, capacity, stability)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-risk-pd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
